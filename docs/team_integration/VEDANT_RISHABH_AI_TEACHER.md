# üéì Vedant/Rishabh's Team: AI Teacher Scoring Integration Guide

## üìã **Overview**

This guide provides **Vedant** and **Rishabh** with everything needed to integrate the Multimodal Sentiment Analysis API for **AI teacher scoring and educational content analysis**. The system is optimized for analyzing student responses, engagement levels, and learning outcomes.

---

## üéØ **Use Case: AI Teacher Scoring**

**Goal**: Analyze student responses (text, audio, video) to provide intelligent scoring and feedback for educational content.

**Key Requirements:**
- **Content accuracy assessment** through text analysis
- **Engagement detection** through audio/video analysis
- **Learning progress tracking** with adaptive scoring
- **Comprehensive feedback generation** for improvement

---

## ‚öôÔ∏è **Optimal Configuration**

### **Fusion Settings (Pre-configured)**
```yaml
# config/fusion_config.yaml
fusion:
  method: "adaptive"             # Learns from scoring patterns
  confidence_threshold: 0.75     # Moderate threshold for education
  enable_dynamic_weights: true   # Adapt to different content types

weights:
  text: 0.6    # Primary: content analysis and accuracy
  audio: 0.3   # Secondary: engagement and clarity
  video: 0.1   # Minimal: visual aids and presentation

# Educational-specific settings
modalities:
  text:
    min_confidence: 0.4
    weight_range: [0.4, 0.8]  # Allow high text focus
    
  audio:
    min_confidence: 0.3
    weight_range: [0.2, 0.5]  # Moderate audio for engagement
```

### **Apply Configuration**
```python
from fusion_config_manager import get_fusion_config_manager

# Apply Vedant/Rishabh's preset
manager = get_fusion_config_manager()
success = manager.apply_team_preset('vedant_teacher_scoring')
print(f"AI teacher scoring config applied: {success}")
```

---

## üöÄ **Integration Examples**

### **Basic Student Response Scoring**
```python
from sdk.python.sentiment_client import SentimentClient

# Initialize client
client = SentimentClient(base_url="http://localhost:8000")

class AITeacherScorer:
    """AI Teacher scoring system"""
    
    def __init__(self):
        self.client = client
        self.scoring_history = []
    
    def score_student_response(self, response_file, question_context=None):
        """Score a student response comprehensively"""
        
        # Analyze multimodal response
        result = self.client.analyze_multimodal(response_file)
        
        # Extract scoring components
        scoring_data = {
            'content_sentiment': self.get_content_analysis(result),
            'engagement_level': self.get_engagement_analysis(result),
            'clarity_score': self.get_clarity_analysis(result),
            'overall_performance': result['fused_sentiment'],
            'confidence': result['confidence'],
            
            # Detailed breakdown
            'text_analysis': self.get_modality_result(result, 'text'),
            'audio_analysis': self.get_modality_result(result, 'audio'),
            'video_analysis': self.get_modality_result(result, 'video'),
            
            # Metadata
            'processing_time': result['processing_time'],
            'model_versions': result['model_version']
        }
        
        # Generate comprehensive score
        final_score = self.calculate_final_score(scoring_data, question_context)
        
        # Store for adaptive learning
        self.scoring_history.append({
            'response': response_file,
            'score': final_score,
            'timestamp': time.time()
        })
        
        return final_score
    
    def get_content_analysis(self, result):
        """Analyze content quality from text"""
        text_result = self.get_modality_result(result, 'text')
        if not text_result:
            return {'quality': 'unknown', 'confidence': 0.0}
        
        # Map sentiment to content quality
        quality_mapping = {
            'positive': 'good',      # Positive engagement with content
            'neutral': 'adequate',   # Basic understanding
            'negative': 'needs_work' # Confusion or frustration
        }
        
        return {
            'quality': quality_mapping.get(text_result['sentiment'], 'unknown'),
            'confidence': text_result['confidence'],
            'details': text_result
        }
    
    def get_engagement_analysis(self, result):
        """Analyze student engagement from audio"""
        audio_result = self.get_modality_result(result, 'audio')
        if not audio_result:
            return {'level': 'unknown', 'confidence': 0.0}
        
        # Map audio sentiment to engagement
        engagement_mapping = {
            'positive': 'high',      # Enthusiastic, engaged
            'neutral': 'moderate',   # Attentive but not excited
            'negative': 'low'        # Disengaged or frustrated
        }
        
        return {
            'level': engagement_mapping.get(audio_result['sentiment'], 'unknown'),
            'confidence': audio_result['confidence'],
            'details': audio_result
        }
    
    def get_clarity_analysis(self, result):
        """Analyze response clarity from overall confidence"""
        clarity_score = result['confidence']
        
        if clarity_score >= 0.8:
            clarity_level = 'very_clear'
        elif clarity_score >= 0.6:
            clarity_level = 'clear'
        elif clarity_score >= 0.4:
            clarity_level = 'somewhat_clear'
        else:
            clarity_level = 'unclear'
        
        return {
            'level': clarity_level,
            'score': clarity_score
        }
    
    def calculate_final_score(self, scoring_data, question_context):
        """Calculate comprehensive final score"""
        
        # Base score from content analysis
        content_quality = scoring_data['content_sentiment']['quality']
        base_scores = {
            'good': 85,
            'adequate': 70,
            'needs_work': 50,
            'unknown': 60
        }
        base_score = base_scores.get(content_quality, 60)
        
        # Engagement bonus/penalty
        engagement_level = scoring_data['engagement_level']['level']
        engagement_modifiers = {
            'high': +10,
            'moderate': 0,
            'low': -10,
            'unknown': 0
        }
        engagement_modifier = engagement_modifiers.get(engagement_level, 0)
        
        # Clarity bonus/penalty
        clarity_level = scoring_data['clarity_score']['level']
        clarity_modifiers = {
            'very_clear': +5,
            'clear': 0,
            'somewhat_clear': -5,
            'unclear': -10
        }
        clarity_modifier = clarity_modifiers.get(clarity_level, 0)
        
        # Calculate final score
        final_score = base_score + engagement_modifier + clarity_modifier
        final_score = max(0, min(100, final_score))  # Clamp to 0-100
        
        return {
            'score': final_score,
            'breakdown': {
                'content_score': base_score,
                'engagement_modifier': engagement_modifier,
                'clarity_modifier': clarity_modifier
            },
            'feedback': self.generate_feedback(scoring_data),
            'recommendations': self.generate_recommendations(scoring_data)
        }
    
    def generate_feedback(self, scoring_data):
        """Generate personalized feedback"""
        feedback = []
        
        # Content feedback
        content_quality = scoring_data['content_sentiment']['quality']
        if content_quality == 'good':
            feedback.append("‚úÖ Excellent understanding of the content!")
        elif content_quality == 'adequate':
            feedback.append("üëç Good grasp of the material, with room for deeper insight.")
        else:
            feedback.append("üìö Consider reviewing the material for better understanding.")
        
        # Engagement feedback
        engagement_level = scoring_data['engagement_level']['level']
        if engagement_level == 'high':
            feedback.append("üåü Great enthusiasm and engagement!")
        elif engagement_level == 'low':
            feedback.append("üí° Try to engage more actively with the material.")
        
        # Clarity feedback
        clarity_level = scoring_data['clarity_score']['level']
        if clarity_level in ['unclear', 'somewhat_clear']:
            feedback.append("üó£Ô∏è Work on expressing your ideas more clearly.")
        
        return feedback
    
    def generate_recommendations(self, scoring_data):
        """Generate improvement recommendations"""
        recommendations = []
        
        # Based on content analysis
        if scoring_data['content_sentiment']['confidence'] < 0.6:
            recommendations.append("Review the core concepts before attempting similar questions")
        
        # Based on engagement
        if scoring_data['engagement_level']['level'] == 'low':
            recommendations.append("Try to find personal connections to the material")
        
        # Based on clarity
        if scoring_data['clarity_score']['score'] < 0.6:
            recommendations.append("Practice explaining concepts in your own words")
        
        return recommendations
    
    def get_modality_result(self, result, modality):
        """Extract specific modality result"""
        for item in result['individual']:
            if item['modality'] == modality:
                return {
                    'sentiment': item['sentiment'],
                    'confidence': item['confidence']
                }
        return None

# Example usage
scorer = AITeacherScorer()
score_result = scorer.score_student_response("student_response.mp4")
print(f"Student Score: {score_result['score']}/100")
print(f"Feedback: {score_result['feedback']}")
```

### **Batch Scoring for Classroom**
```python
def score_classroom_responses(response_files, question_id):
    """Score multiple student responses efficiently"""
    
    scorer = AITeacherScorer()
    classroom_results = []
    
    for i, response_file in enumerate(response_files):
        print(f"Scoring response {i+1}/{len(response_files)}...")
        
        try:
            score_result = scorer.score_student_response(response_file)
            classroom_results.append({
                'student_id': f"student_{i+1}",
                'response_file': response_file,
                'score': score_result['score'],
                'feedback': score_result['feedback'],
                'recommendations': score_result['recommendations']
            })
        except Exception as e:
            print(f"Error scoring {response_file}: {e}")
            classroom_results.append({
                'student_id': f"student_{i+1}",
                'response_file': response_file,
                'error': str(e)
            })
    
    # Generate classroom analytics
    analytics = generate_classroom_analytics(classroom_results)
    
    return {
        'individual_scores': classroom_results,
        'classroom_analytics': analytics
    }

def generate_classroom_analytics(results):
    """Generate classroom-level analytics"""
    valid_scores = [r['score'] for r in results if 'score' in r]
    
    if not valid_scores:
        return {'error': 'No valid scores to analyze'}
    
    return {
        'average_score': sum(valid_scores) / len(valid_scores),
        'highest_score': max(valid_scores),
        'lowest_score': min(valid_scores),
        'total_responses': len(results),
        'successful_analyses': len(valid_scores),
        'score_distribution': {
            'excellent': len([s for s in valid_scores if s >= 90]),
            'good': len([s for s in valid_scores if 80 <= s < 90]),
            'satisfactory': len([s for s in valid_scores if 70 <= s < 80]),
            'needs_improvement': len([s for s in valid_scores if s < 70])
        }
    }
```

### **Adaptive Learning Integration**
```python
class AdaptiveLearningSystem:
    """Adaptive learning system that improves over time"""
    
    def __init__(self):
        self.learning_history = []
        self.performance_metrics = {}
    
    def track_learning_progress(self, student_id, scores_over_time):
        """Track student learning progress"""
        
        if len(scores_over_time) < 2:
            return {'status': 'insufficient_data'}
        
        # Calculate learning trend
        recent_scores = scores_over_time[-5:]  # Last 5 scores
        early_scores = scores_over_time[:5]    # First 5 scores
        
        recent_avg = sum(recent_scores) / len(recent_scores)
        early_avg = sum(early_scores) / len(early_scores)
        
        improvement = recent_avg - early_avg
        
        # Determine learning status
        if improvement > 10:
            status = 'improving_rapidly'
        elif improvement > 5:
            status = 'improving_steadily'
        elif improvement > -5:
            status = 'stable'
        else:
            status = 'declining'
        
        return {
            'student_id': student_id,
            'status': status,
            'improvement': improvement,
            'recent_average': recent_avg,
            'early_average': early_avg,
            'total_responses': len(scores_over_time)
        }
    
    def adjust_difficulty(self, student_progress):
        """Suggest difficulty adjustments based on progress"""
        
        status = student_progress['status']
        recent_avg = student_progress['recent_average']
        
        if status == 'improving_rapidly' and recent_avg > 85:
            return 'increase_difficulty'
        elif status == 'declining' or recent_avg < 60:
            return 'decrease_difficulty'
        else:
            return 'maintain_difficulty'
```

---

## üéõÔ∏è **Configuration Tuning**

### **For Different Subject Areas**

**STEM Subjects (Math, Science):**
```python
# Higher text weight for technical accuracy
manager.update_weights({
    'text': 0.7,   # Focus on technical content
    'audio': 0.2,  # Minimal audio
    'video': 0.1   # Minimal video
})
```

**Language Arts:**
```python
# Balanced weights for expression and content
manager.update_weights({
    'text': 0.5,   # Content analysis
    'audio': 0.4,  # Verbal expression
    'video': 0.1   # Presentation
})
```

**Presentation Skills:**
```python
# Higher audio/video for delivery assessment
manager.update_weights({
    'text': 0.3,   # Content
    'audio': 0.4,  # Vocal delivery
    'video': 0.3   # Visual presentation
})
```

---

## üìä **Performance Monitoring**

```python
def monitor_scoring_performance():
    """Monitor AI teacher scoring performance"""
    
    # Get recent scoring statistics
    recent_scores = get_recent_scores(days=7)
    
    metrics = {
        'total_responses_scored': len(recent_scores),
        'average_processing_time': calculate_avg_processing_time(recent_scores),
        'score_distribution': calculate_score_distribution(recent_scores),
        'confidence_levels': calculate_confidence_distribution(recent_scores)
    }
    
    return metrics
```

---

## üîß **Troubleshooting**

### **Common Issues**

**Low Text Confidence:**
```python
# Check if text content is too short or unclear
if text_result['confidence'] < 0.5:
    print("Consider asking for more detailed responses")
```

**Inconsistent Scoring:**
```python
# Monitor score variance for similar responses
if score_variance > threshold:
    print("Consider adjusting fusion weights for consistency")
```

---

## ‚úÖ **Quick Checklist**

- [ ] Applied `vedant_teacher_scoring` preset
- [ ] Tested with sample student responses
- [ ] Configured subject-specific weights
- [ ] Set up batch processing for classrooms
- [ ] Implemented adaptive learning tracking
- [ ] Created feedback generation system

**üéì Ready to enhance education with AI-powered scoring!**
