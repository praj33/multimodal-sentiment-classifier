#!/usr/bin/env python3
"""
ðŸŽ­ Multimodal Sentiment Analysis API

A comprehensive, enterprise-grade FastAPI application for analyzing sentiment from text, audio,
and video inputs using state-of-the-art AI models and sophisticated fusion techniques.

ðŸš€ Features:
- ðŸ“ Text sentiment analysis using DistilBERT transformer models
- ðŸŽµ Audio sentiment analysis using MFCC feature extraction
- ðŸŽ¥ Video sentiment analysis using MediaPipe facial recognition
- âš¡ Advanced multimodal fusion with confidence weighting
- ðŸ“¡ Real-time streaming analysis with WebSocket support
- ðŸŒ Interactive web dashboard with drag-and-drop interface
- ðŸ“Š Performance monitoring and comprehensive analytics
- ðŸ›¡ï¸ Enterprise-grade security with input validation
- ðŸ³ Production-ready Docker deployment
- âš™ï¸ Runtime configuration with hot-reload capability

ðŸ‘¥ Team Integration Ready:
- Gandhar (Avatar Emotions): Optimized for emotional nuance
- Vedant/Rishabh (AI Teacher): Optimized for educational content
- Shashank (Content Moderation): Optimized for safety detection

Author: praj33
Version: 1.0.0 (Day 3 Complete - Production Ready)
License: MIT
"""

from fastapi import FastAPI, UploadFile, File, HTTPException, Request, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, HTMLResponse, StreamingResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import List
from input_validation import input_validator
from streaming_api import add_streaming_routes, STREAMING_TEST_HTML
# Classifier imports moved to lazy loading functions to prevent startup hanging
from fusion.fusion_engine import FusionEngine
from enhanced_logging import EnhancedSentimentLogger
from fusion_config_manager import get_fusion_config_manager

# Day 2-3: Import configuration and validation modules
from config_loader import get_config_loader
from model_versioning import format_api_response, format_multimodal_response, get_version_manager
from validation_middleware import configure_validation_middleware, RequestValidationHelper

# Import analytics dashboard
from advanced_analytics_dashboard import analytics_engine, AnalyticsMetric

import os
import yaml
import time
from datetime import datetime

app = FastAPI(
    title="Multimodal Sentiment Analysis API",
    description="Analyze sentiment from text, audio, and video using AI models. Visit /dashboard for the web interface.",
    version="1.0.0"
)

# Day 2: Configure enhanced validation middleware
app = configure_validation_middleware(app)

# Add security middleware
@app.middleware("http")
async def security_middleware(request: Request, call_next):
    """Basic security middleware"""
    # Add security headers
    response = await call_next(request)
    response.headers["X-Content-Type-Options"] = "nosniff"
    response.headers["X-Frame-Options"] = "DENY"
    response.headers["X-XSS-Protection"] = "1; mode=block"
    response.headers["Referrer-Policy"] = "strict-origin-when-cross-origin"
    return response

# Load config with environment variable support
config_loader = get_config_loader()
config = config_loader.get_config()

# Initialize model versioning system (Day 2 requirement)
from model_versioning import ModelVersionManager
version_manager = ModelVersionManager()
# response_formatter is now handled by format_api_response functions

# Print configuration summary for debugging
if os.getenv('DEBUG', 'false').lower() == 'true':
    config_loader.print_config_summary()

# Initialize models with lazy loading (FIXED: prevents 30+ second startup hanging)
text_model = None
audio_model = None
video_model = None
fusion_engine = None

def get_text_model():
    """Lazy load text model (takes 15-20 seconds)"""
    global text_model
    if text_model is None:
        print("ðŸ§  Loading TextClassifier (this may take 15-20 seconds)...")
        from classifiers.text_classifier import TextClassifier
        text_model = TextClassifier()
        print("âœ… TextClassifier loaded")
    return text_model

def get_audio_model():
    """Lazy load audio model"""
    global audio_model
    if audio_model is None:
        print("ðŸŽµ Loading AudioClassifier...")
        from classifiers.audio_classifier import AudioClassifier
        audio_model = AudioClassifier()
        print("âœ… AudioClassifier loaded")
    return audio_model

def get_video_model():
    """Lazy load video model (takes 5-10 seconds)"""
    global video_model
    if video_model is None:
        print("ðŸŽ¥ Loading VideoClassifier (this may take 5-10 seconds)...")
        from classifiers.video_classifier import VideoClassifier
        video_model = VideoClassifier()
        print("âœ… VideoClassifier loaded")
    return video_model

def get_fusion_engine():
    """Lazy load fusion engine"""
    global fusion_engine
    if fusion_engine is None:
        print("âš¡ Loading FusionEngine...")
        from fusion.fusion_engine import FusionEngine
        fusion_engine = FusionEngine()
        print("âœ… FusionEngine loaded")
    return fusion_engine

# Initialize enhanced logger
sentiment_logger = EnhancedSentimentLogger()

# Analytics helper function
async def log_analytics_metric(sentiment: str, confidence: float, modality: str,
                              processing_time: float, user_id: str = None,
                              location: str = None, session_id: str = None):
    """Log analytics metric for dashboard tracking"""
    try:
        metric = AnalyticsMetric(
            timestamp=datetime.now(),
            sentiment=sentiment,
            confidence=confidence,
            modality=modality,
            processing_time=processing_time,
            user_id=user_id,
            location=location,
            session_id=session_id
        )
        await analytics_engine.add_metric(metric)
    except Exception as e:
        print(f"Failed to log analytics metric: {e}")

# Request model for text
class TextInput(BaseModel):
    text: str

@app.get("/")
def root():
    return {"message": "Multimodal Sentiment Classifier API is running."}

# Import the enhanced dashboard HTML
from multimodal_dashboard import MULTIMODAL_DASHBOARD_HTML

# Serve the enhanced multimodal dashboard
@app.get("/dashboard", response_class=HTMLResponse)
def get_dashboard():
    """Serve the enhanced multimodal dashboard"""
    return HTMLResponse(content=MULTIMODAL_DASHBOARD_HTML)

# Serve static files (CSS, JS)
@app.get("/frontend/{file_path}")
def get_static_file(file_path: str):
    """Serve static frontend files"""
    file_location = f"frontend/{file_path}"
    if os.path.exists(file_location):
        return FileResponse(file_location)
    else:
        return {"error": "File not found"}

# Health check endpoint (Day 2: enhanced with version info)
@app.get("/health")
def health_check():
    return {
        "status": "healthy",
        "timestamp": time.time(),
        "model_version": version_manager.get_model_version_dict(),
        "system": "multimodal_sentiment_analysis"
    }

# Analytics endpoint
@app.get("/analytics")
def get_analytics():
    """Get system analytics"""
    try:
        from enhanced_logging import EnhancedSentimentLogger
        logger = EnhancedSentimentLogger()
        analytics = logger.get_analytics()
        return analytics
    except Exception as e:
        return {"error": f"Analytics not available: {str(e)}"}

@app.post("/predict/text",
    summary="Text Sentiment Analysis",
    description="""
    Analyze sentiment from text input with enhanced Day 2 validation.

    **Validation Rules:**
    - Text length: 1-10,000 characters
    - XSS protection and sanitization applied
    - Malicious content detection

    **Response includes model versioning (Day 2):**
    - Model version information for text classifier
    - Processing time and prediction ID
    """,
    response_description="Sentiment prediction with model version info")
def predict_text(data: TextInput):
    if not config["models"]["text"]["enabled"]:
        return {"error": "Text model disabled in config"}

    # Validate and sanitize input text
    try:
        sanitized_text = input_validator.validate_text_input(data.text)
    except HTTPException as e:
        raise e

    start_time = time.time()
    # Get advanced sentiment analysis result
    analysis_result = get_text_model().predict(sanitized_text)
    processing_time = time.time() - start_time

    # Extract basic sentiment and confidence for compatibility
    if isinstance(analysis_result, dict):
        sentiment = analysis_result.get('sentiment', 'neutral')
        confidence = analysis_result.get('confidence', 0.5)
        advanced_data = analysis_result
    else:
        # Fallback for basic response format
        sentiment, confidence = analysis_result
        advanced_data = {'sentiment': sentiment, 'confidence': confidence}

    # Log the prediction
    prediction_id = sentiment_logger.log_prediction(
        mode="text",
        result=advanced_data,
        confidence=confidence,
        input_content=sanitized_text,
        processing_time=processing_time * 1000
    )

    # Log analytics metric for dashboard
    import asyncio
    try:
        asyncio.create_task(log_analytics_metric(
            sentiment=sentiment,
            confidence=confidence,
            modality="text",
            processing_time=processing_time * 1000
        ))
    except Exception as e:
        print(f"Analytics logging failed: {e}")

    # Create advanced response with model versioning
    response = format_api_response(
        sentiment=sentiment,
        confidence=confidence,
        used_models=["text"],
        prediction_id=prediction_id,
        processing_time=processing_time * 1000
    )

    # Add advanced analysis data
    if isinstance(analysis_result, dict) and analysis_result.get('advanced_analysis'):
        response.update({
            'emotions': analysis_result.get('emotions', {}),
            'intensity': analysis_result.get('intensity', 'medium'),
            'emotional_context': analysis_result.get('emotional_context', {}),
            'advanced_metrics': analysis_result.get('advanced_metrics', {}),
            'analysis_type': 'advanced'
        })
    else:
        response['analysis_type'] = 'basic'

    return response

@app.post("/predict/text/advanced",
    summary="ðŸ§  Advanced Text Sentiment Analysis",
    description="""
    **ADVANCED** text sentiment analysis with comprehensive emotion detection and psychological insights.

    **ðŸŽ¯ Advanced Features:**
    - **Emotion Detection**: Joy, sadness, anger, fear, surprise, disgust
    - **Intensity Analysis**: Low, medium, high, extreme emotional intensity
    - **Emotional Complexity**: Mixed emotion detection and analysis
    - **Psychological Metrics**: Emotional stability, consistency, strength
    - **Context Analysis**: Dominant and secondary emotions
    - **Sentiment Strength**: Precise confidence scoring

    **ðŸ“Š Response Format:**
    ```json
    {
      "sentiment": "positive",
      "confidence": 0.88,
      "emotions": {
        "joy": 0.85,
        "excitement": 0.72,
        "satisfaction": 0.68
      },
      "intensity": "high",
      "emotional_context": {
        "dominant_emotion": "joy",
        "secondary_emotion": "excitement",
        "emotional_stability": 0.82
      },
      "advanced_metrics": {
        "emotional_complexity": 0.45,
        "sentiment_strength": 0.88,
        "emotional_consistency": 0.82
      }
    }
    ```
    """,
    response_description="Advanced sentiment analysis with emotion detection and psychological insights")
def predict_text_advanced(data: TextInput):
    """Advanced text sentiment analysis with comprehensive emotion detection"""
    if not config["models"]["text"]["enabled"]:
        return {"error": "Text model disabled in config"}

    # Validate and sanitize input text
    try:
        sanitized_text = input_validator.validate_text_input(data.text)
    except HTTPException as e:
        raise e

    start_time = time.time()
    # Force advanced analysis
    analysis_result = get_text_model().predict(sanitized_text)
    processing_time = time.time() - start_time

    # Ensure we get advanced analysis
    if isinstance(analysis_result, dict) and analysis_result.get('advanced_analysis'):
        sentiment = analysis_result.get('sentiment', 'neutral')
        confidence = analysis_result.get('confidence', 0.5)

        # Log the prediction
        prediction_id = sentiment_logger.log_prediction(
            mode="text_advanced",
            result=analysis_result,
            confidence=confidence,
            input_content=sanitized_text,
            processing_time=processing_time * 1000
        )

        # Create comprehensive advanced response
        response = format_api_response(
            sentiment=sentiment,
            confidence=confidence,
            used_models=["text"],
            prediction_id=prediction_id,
            processing_time=processing_time * 1000,
            analysis_type="advanced"
        )

        # Add all advanced features
        response.update({
            'emotions': analysis_result.get('emotions', {}),
            'intensity': analysis_result.get('intensity', 'medium'),
            'emotional_context': analysis_result.get('emotional_context', {}),
            'advanced_metrics': analysis_result.get('advanced_metrics', {}),
            'psychological_insights': {
                'emotional_range': len(analysis_result.get('emotions', {})),
                'primary_emotion_strength': max(analysis_result.get('emotions', {}).values()) if analysis_result.get('emotions') else 0.5,
                'emotional_balance': analysis_result.get('advanced_metrics', {}).get('emotional_complexity', 0.0)
            }
        })

        return response
    else:
        raise HTTPException(status_code=503, detail="Advanced analysis not available")

@app.post("/predict/emotions",
    summary="ðŸŽ­ Pure Emotion Detection",
    description="""
    **EMOTION-FOCUSED** analysis that identifies specific emotions beyond basic sentiment.

    **ðŸŽ¯ Emotion Categories:**
    - **Primary**: Joy, Sadness, Anger, Fear, Surprise, Disgust
    - **Secondary**: Excitement, Frustration, Anxiety, Contentment, etc.
    - **Intensity Levels**: Low (0.0-0.3), Medium (0.3-0.7), High (0.7-0.9), Extreme (0.9-1.0)

    **ðŸ“Š Use Cases:**
    - Avatar emotion mapping
    - Customer feedback analysis
    - Mental health monitoring
    - Content emotional profiling
    """,
    response_description="Detailed emotion detection with intensity and context")
def predict_emotions(data: TextInput):
    """Pure emotion detection and analysis"""
    if not config["models"]["text"]["enabled"]:
        return {"error": "Text model disabled in config"}

    try:
        sanitized_text = input_validator.validate_text_input(data.text)
    except HTTPException as e:
        raise e

    start_time = time.time()
    analysis_result = get_text_model().predict(sanitized_text)
    processing_time = time.time() - start_time

    if isinstance(analysis_result, dict) and analysis_result.get('emotions'):
        # Log the prediction
        prediction_id = sentiment_logger.log_prediction(
            mode="emotions",
            result=analysis_result,
            confidence=analysis_result.get('confidence', 0.5),
            input_content=sanitized_text,
            processing_time=processing_time * 1000
        )

        # Create emotion-focused response
        emotions = analysis_result.get('emotions', {})
        dominant_emotion = analysis_result.get('emotional_context', {}).get('dominant_emotion', 'neutral')

        response = format_api_response(
            sentiment=dominant_emotion,  # Use dominant emotion as sentiment
            confidence=max(emotions.values()) if emotions else 0.5,
            used_models=["text"],
            prediction_id=prediction_id,
            processing_time=processing_time * 1000,
            analysis_type="emotion_detection"
        )

        # Add emotion-specific data
        response.update({
            'emotions': emotions,
            'emotion_ranking': sorted(emotions.items(), key=lambda x: x[1], reverse=True),
            'intensity': analysis_result.get('intensity', 'medium'),
            'emotional_context': analysis_result.get('emotional_context', {}),
            'emotion_summary': {
                'primary_emotion': dominant_emotion,
                'emotion_count': len(emotions),
                'strongest_emotion_score': max(emotions.values()) if emotions else 0.0,
                'emotional_diversity': len([e for e in emotions.values() if e > 0.3])
            }
        })

        return response
    else:
        raise HTTPException(status_code=503, detail="Emotion detection not available")

@app.post("/predict/audio",
    summary="Audio Sentiment Analysis",
    description="""
    Analyze sentiment from audio files with Day 2 enhanced validation.

    **Supported Formats:** WAV, MP3, OGG, M4A
    **File Size Limit:** 50MB maximum
    **Validation Features:**
    - Magic number verification
    - File header validation
    - Content-type checking

    **Response includes model versioning (Day 2):**
    - Audio model version information
    - File metadata and processing metrics
    """,
    response_description="Audio sentiment prediction with model version info")
async def predict_audio(file: UploadFile = File(...)):
    if not config["models"]["audio"]["enabled"]:
        return {"error": "Audio model disabled in config"}

    # Validate uploaded file
    try:
        input_validator.validate_file_upload(file, "audio")
    except HTTPException as e:
        raise e

    start_time = time.time()
    contents = await file.read()
    safe_filename = input_validator.sanitize_filename(file.filename)
    temp_path = f"temp_{safe_filename}"
    with open(temp_path, "wb") as f:
        f.write(contents)

    sentiment, score = get_audio_model().predict(temp_path)
    processing_time = time.time() - start_time
    os.remove(temp_path)

    # Log the prediction
    prediction_id = sentiment_logger.log_prediction(
        mode="audio",
        result={"sentiment": sentiment, "confidence": score},
        confidence=score,
        input_data={"file_size": len(contents), "filename": file.filename},
        processing_time=processing_time * 1000  # Convert to milliseconds
    )

    # Day 2: Use EXACT response format with model versioning (CRITICAL requirement)
    return format_api_response(
        sentiment=sentiment,
        confidence=score,
        used_models=["audio"],
        prediction_id=prediction_id,
        processing_time=processing_time * 1000,
        file_info={"filename": file.filename, "size": len(contents)}
    )

@app.post("/predict/video",
    summary="Video Sentiment Analysis",
    description="""
    Analyze sentiment from video files with Day 2 enhanced validation.

    **Supported Formats:** MP4, MOV, AVI
    **File Size Limit:** 50MB maximum
    **Validation Features:**
    - Magic number verification
    - File header validation
    - Content-type checking

    **Response includes model versioning (Day 2):**
    - Video model version information
    - File metadata and processing metrics
    """,
    response_description="Video sentiment prediction with model version info")
async def predict_video(file: UploadFile = File(...)):
    if not config["models"]["video"]["enabled"]:
        return {"error": "Video model disabled in config"}

    # Validate uploaded file
    try:
        input_validator.validate_file_upload(file, "video")
    except HTTPException as e:
        raise e

    start_time = time.time()
    contents = await file.read()
    safe_filename = input_validator.sanitize_filename(file.filename)
    temp_path = f"temp_{safe_filename}"
    with open(temp_path, "wb") as f:
        f.write(contents)

    sentiment, score = get_video_model().predict(temp_path)
    processing_time = time.time() - start_time
    os.remove(temp_path)

    # Log the prediction
    prediction_id = sentiment_logger.log_prediction(
        mode="video",
        result={"sentiment": sentiment, "confidence": score},
        confidence=score,
        input_data={"file_size": len(contents), "filename": file.filename},
        processing_time=processing_time * 1000  # Convert to milliseconds
    )

    # Day 2: Use EXACT response format with model versioning (CRITICAL requirement)
    return format_api_response(
        sentiment=sentiment,
        confidence=score,
        used_models=["video"],
        prediction_id=prediction_id,
        processing_time=processing_time * 1000,
        file_info={"filename": file.filename, "size": len(contents)}
    )

@app.post("/predict/multimodal",
    summary="Multimodal Sentiment Analysis",
    description="""
    Analyze sentiment using multiple modalities with advanced fusion techniques.

    **Supported Files:** Audio (WAV, MP3, OGG, M4A) or Video (MP4, MOV, AVI)
    **File Size Limit:** 50MB maximum
    **Processing:** Analyzes all applicable modalities and fuses results

    **Day 2 Enhanced Features:**
    - Comprehensive file validation
    - Magic number verification
    - Advanced fusion algorithms

    **Response includes complete model versioning:**
    - Individual modality versions
    - Fusion algorithm version
    - Individual and fused results
    """,
    response_description="Multimodal sentiment prediction with complete model version info")
async def predict_multimodal(file: UploadFile = File(...)):
    # Validate uploaded file (try video first, then audio)
    try:
        input_validator.validate_file_upload(file, "video")
    except HTTPException:
        try:
            input_validator.validate_file_upload(file, "audio")
        except HTTPException:
            raise HTTPException(status_code=400, detail="File must be a valid audio or video file")

    start_time = time.time()
    results = []
    modalities = []

    contents = await file.read()
    safe_filename = input_validator.sanitize_filename(file.filename)
    temp_path = f"temp_{safe_filename}"
    with open(temp_path, "wb") as f:
        f.write(contents)

    # Process each enabled modality
    if config["models"]["text"]["enabled"]:
        sentiment, score = text_model.predict("This is a great example!")  # dummy input for now
        results.append((sentiment, score))
        modalities.append("text")

    if config["models"]["audio"]["enabled"]:
        sentiment, score = get_audio_model().predict(temp_path)
        results.append((sentiment, score))
        modalities.append("audio")

    if config["models"]["video"]["enabled"]:
        sentiment, score = get_video_model().predict(temp_path)
        results.append((sentiment, score))
        modalities.append("video")

    os.remove(temp_path)
    processing_time = time.time() - start_time

    # Use enhanced fusion with modality information
    final_sentiment, final_confidence = get_fusion_engine().predict(results, modalities)

    # Prepare individual results for response
    individual_results = [
        {"modality": modalities[i], "sentiment": results[i][0], "confidence": results[i][1]}
        for i in range(len(results))
    ]

    # Log the multimodal prediction
    prediction_id = sentiment_logger.log_prediction(
        mode="multimodal",
        result={
            "sentiment": final_sentiment,
            "confidence": final_confidence,
            "individual_results": individual_results
        },
        confidence=final_confidence,
        input_data={"filename": file.filename, "modalities": modalities},
        processing_time=processing_time * 1000  # Convert to milliseconds
    )

    # Day 2: Use EXACT response format with model versioning (CRITICAL requirement)
    used_models = modalities + ["fusion"]  # Include fusion in used models
    return format_multimodal_response(
        fused_sentiment=final_sentiment,
        fused_confidence=final_confidence,
        individual_results=individual_results,
        used_models=used_models,
        prediction_id=prediction_id,
        processing_time=processing_time * 1000
    )

@app.post("/predict/multimodal/advanced",
    summary="ðŸš€ Advanced Multimodal Analysis",
    description="""
    **ENTERPRISE-GRADE** multimodal sentiment analysis with comprehensive insights and fusion analytics.

    **ðŸŽ¯ Advanced Multimodal Features:**
    - **Deep Fusion Analysis**: Confidence-weighted, consensus-based, adaptive fusion
    - **Modality Insights**: Individual performance metrics for each input type
    - **Fusion Confidence**: How well different modalities agree with each other
    - **Conflict Detection**: Identifies when modalities disagree and why
    - **Weighted Contributions**: Shows which modality influenced the final result most
    - **Temporal Analysis**: Processing time breakdown by modality
    - **Quality Metrics**: Input quality assessment for each modality

    **ðŸ“Š Advanced Response Format:**
    ```json
    {
      "sentiment": "positive",
      "confidence": 0.88,
      "fusion_analysis": {
        "fusion_method": "confidence_weighted",
        "consensus_level": 0.85,
        "conflict_detected": false,
        "modality_agreement": {
          "text_audio": 0.92,
          "text_video": 0.78,
          "audio_video": 0.83
        }
      },
      "modality_contributions": {
        "text": {"weight": 0.45, "influence": 0.52},
        "audio": {"weight": 0.35, "influence": 0.28},
        "video": {"weight": 0.20, "influence": 0.20}
      },
      "individual_analysis": [
        {
          "modality": "text",
          "sentiment": "positive",
          "confidence": 0.92,
          "emotions": {"joy": 0.85, "excitement": 0.72},
          "quality_score": 0.95
        }
      ]
    }
    ```

    **ðŸŽ¯ Use Cases:**
    - High-stakes decision making
    - Research and analytics
    - Quality assurance
    - Model performance analysis
    """,
    response_description="Advanced multimodal analysis with fusion insights and detailed metrics")
async def predict_multimodal_advanced(
    text: str = Form(None),
    audio: UploadFile = File(None),
    video: UploadFile = File(None)
):
    """Advanced multimodal sentiment analysis with comprehensive fusion insights"""

    if not any([text, audio, video]):
        raise HTTPException(status_code=400, detail="At least one input (text, audio, or video) is required")

    start_time = time.time()
    individual_results = []
    modality_timings = {}
    quality_scores = {}

    # Process text if provided
    if text:
        text_start = time.time()
        try:
            sanitized_text = input_validator.validate_text_input(text)
            text_result = text_model.predict(sanitized_text)

            if isinstance(text_result, dict):
                individual_results.append({
                    "modality": "text",
                    "sentiment": text_result.get('sentiment', 'neutral'),
                    "confidence": text_result.get('confidence', 0.5),
                    "emotions": text_result.get('emotions', {}),
                    "intensity": text_result.get('intensity', 'medium'),
                    "emotional_context": text_result.get('emotional_context', {}),
                    "quality_score": 0.95  # Text quality is generally high
                })
                quality_scores['text'] = 0.95
            else:
                sentiment, confidence = text_result
                individual_results.append({
                    "modality": "text",
                    "sentiment": sentiment,
                    "confidence": confidence,
                    "quality_score": 0.85
                })
                quality_scores['text'] = 0.85

        except Exception as e:
            individual_results.append({
                "modality": "text",
                "sentiment": "neutral",
                "confidence": 0.5,
                "error": str(e),
                "quality_score": 0.0
            })
            quality_scores['text'] = 0.0

        modality_timings['text'] = time.time() - text_start

    # Process audio if provided
    if audio:
        audio_start = time.time()
        try:
            file_info = input_validator.validate_file_upload(audio, "audio")
            contents = await audio.read()
            audio_result = get_audio_model().predict(contents)

            if isinstance(audio_result, dict):
                individual_results.append({
                    "modality": "audio",
                    "sentiment": audio_result.get('sentiment', 'neutral'),
                    "confidence": audio_result.get('confidence', 0.5),
                    "file_info": file_info,
                    "quality_score": min(1.0, len(contents) / (1024 * 1024))  # Quality based on file size
                })
                quality_scores['audio'] = min(1.0, len(contents) / (1024 * 1024))
            else:
                sentiment, confidence = audio_result
                individual_results.append({
                    "modality": "audio",
                    "sentiment": sentiment,
                    "confidence": confidence,
                    "file_info": file_info,
                    "quality_score": 0.7
                })
                quality_scores['audio'] = 0.7

        except Exception as e:
            individual_results.append({
                "modality": "audio",
                "sentiment": "neutral",
                "confidence": 0.5,
                "error": str(e),
                "quality_score": 0.0
            })
            quality_scores['audio'] = 0.0

        modality_timings['audio'] = time.time() - audio_start

    # Process video if provided
    if video:
        video_start = time.time()
        try:
            file_info = input_validator.validate_file_upload(video, "video")
            contents = await video.read()
            video_result = get_video_model().predict(contents)

            if isinstance(video_result, dict):
                individual_results.append({
                    "modality": "video",
                    "sentiment": video_result.get('sentiment', 'neutral'),
                    "confidence": video_result.get('confidence', 0.5),
                    "file_info": file_info,
                    "quality_score": min(1.0, len(contents) / (5 * 1024 * 1024))  # Quality based on file size
                })
                quality_scores['video'] = min(1.0, len(contents) / (5 * 1024 * 1024))
            else:
                sentiment, confidence = video_result
                individual_results.append({
                    "modality": "video",
                    "sentiment": sentiment,
                    "confidence": confidence,
                    "file_info": file_info,
                    "quality_score": 0.7
                })
                quality_scores['video'] = 0.7

        except Exception as e:
            individual_results.append({
                "modality": "video",
                "sentiment": "neutral",
                "confidence": 0.5,
                "error": str(e),
                "quality_score": 0.0
            })
            quality_scores['video'] = 0.0

        modality_timings['video'] = time.time() - video_start

    # Advanced fusion analysis
    fusion_start = time.time()

    # Extract sentiments and confidences for fusion
    predictions = [(result['sentiment'], result['confidence']) for result in individual_results if 'error' not in result]
    modalities = [result['modality'] for result in individual_results if 'error' not in result]

    if predictions:
        # Use fusion engine for advanced analysis
        fused_sentiment, fused_confidence = get_fusion_engine().predict(predictions, modalities)

        # Calculate advanced fusion metrics
        fusion_analysis = {
            "fusion_method": get_fusion_engine().fusion_method,
            "consensus_level": calculate_consensus_level(predictions),
            "conflict_detected": detect_conflicts(predictions),
            "modality_agreement": calculate_modality_agreement(individual_results),
            "fusion_confidence": fused_confidence
        }

        # Calculate modality contributions
        modality_contributions = calculate_modality_contributions(individual_results, get_fusion_engine().base_weights)

    else:
        fused_sentiment, fused_confidence = "neutral", 0.5
        fusion_analysis = {"error": "No valid predictions to fuse"}
        modality_contributions = {}

    fusion_timing = time.time() - fusion_start
    total_processing_time = time.time() - start_time

    # Log the advanced prediction
    prediction_id = sentiment_logger.log_prediction(
        mode="multimodal_advanced",
        result={
            "sentiment": fused_sentiment,
            "confidence": fused_confidence,
            "individual": individual_results,
            "fusion_analysis": fusion_analysis
        },
        confidence=fused_confidence,
        input_content=f"multimodal: text={bool(text)}, audio={bool(audio)}, video={bool(video)}",
        processing_time=total_processing_time * 1000
    )

    # Create comprehensive advanced response
    response = format_multimodal_response(
        fused_sentiment=fused_sentiment,
        fused_confidence=fused_confidence,
        individual_results=individual_results,
        used_models=[result['modality'] for result in individual_results],
        prediction_id=prediction_id,
        processing_time=total_processing_time * 1000,
        analysis_type="advanced_multimodal"
    )

    # Add advanced analysis data
    response.update({
        "fusion_analysis": fusion_analysis,
        "modality_contributions": modality_contributions,
        "performance_metrics": {
            "total_processing_time": total_processing_time * 1000,
            "modality_timings": {k: v * 1000 for k, v in modality_timings.items()},
            "fusion_time": fusion_timing * 1000,
            "quality_scores": quality_scores
        },
        "system_insights": {
            "modalities_processed": len(individual_results),
            "successful_modalities": len([r for r in individual_results if 'error' not in r]),
            "average_confidence": sum(r['confidence'] for r in individual_results) / len(individual_results) if individual_results else 0,
            "confidence_variance": calculate_confidence_variance(individual_results)
        }
    })

    return response

# Helper functions for advanced analysis
def calculate_consensus_level(predictions):
    """Calculate how much the modalities agree"""
    if len(predictions) < 2:
        return 1.0

    sentiments = [p[0] for p in predictions]
    unique_sentiments = set(sentiments)

    if len(unique_sentiments) == 1:
        return 1.0
    elif len(unique_sentiments) == 2:
        return 0.5
    else:
        return 0.0

def detect_conflicts(predictions):
    """Detect if modalities strongly disagree"""
    if len(predictions) < 2:
        return False

    sentiments = [p[0] for p in predictions]
    confidences = [p[1] for p in predictions]

    # Check for high-confidence disagreement
    high_conf_predictions = [(s, c) for s, c in zip(sentiments, confidences) if c > 0.7]

    if len(high_conf_predictions) >= 2:
        unique_sentiments = set(s for s, c in high_conf_predictions)
        return len(unique_sentiments) > 1

    return False

def calculate_modality_agreement(individual_results):
    """Calculate pairwise agreement between modalities"""
    agreements = {}

    modalities = [r['modality'] for r in individual_results if 'error' not in r]

    for i, mod1 in enumerate(modalities):
        for j, mod2 in enumerate(modalities[i+1:], i+1):
            result1 = individual_results[i]
            result2 = individual_results[j]

            # Simple agreement based on sentiment match and confidence similarity
            sentiment_match = 1.0 if result1['sentiment'] == result2['sentiment'] else 0.0
            confidence_similarity = 1.0 - abs(result1['confidence'] - result2['confidence'])

            agreement = (sentiment_match + confidence_similarity) / 2
            agreements[f"{mod1}_{mod2}"] = agreement

    return agreements

def calculate_modality_contributions(individual_results, base_weights):
    """Calculate how much each modality contributed to the final result"""
    contributions = {}

    total_weight = 0
    for result in individual_results:
        if 'error' not in result:
            modality = result['modality']
            weight = base_weights.get(modality, 1.0) * result['confidence']
            total_weight += weight

    for result in individual_results:
        if 'error' not in result:
            modality = result['modality']
            weight = base_weights.get(modality, 1.0)
            influence = (weight * result['confidence']) / total_weight if total_weight > 0 else 0

            contributions[modality] = {
                "weight": weight,
                "influence": influence,
                "confidence": result['confidence']
            }

    return contributions

def calculate_confidence_variance(individual_results):
    """Calculate variance in confidence scores"""
    confidences = [r['confidence'] for r in individual_results if 'error' not in r]

    if len(confidences) < 2:
        return 0.0

    mean_conf = sum(confidences) / len(confidences)
    variance = sum((c - mean_conf) ** 2 for c in confidences) / len(confidences)

    return variance

# Logging and Analytics Endpoints
@app.get("/analytics/stats")
def get_analytics():
    """Get prediction statistics"""
    try:
        return sentiment_logger.get_analytics()
    except Exception as e:
        return {"error": f"Analytics not available: {str(e)}"}

@app.get("/analytics/predictions")
def get_predictions(limit: int = 50, mode: str = None, sentiment: str = None):
    """Get recent predictions with optional filtering"""
    try:
        # Basic implementation - return recent predictions
        return {"message": "Predictions endpoint available", "limit": limit, "mode": mode, "sentiment": sentiment}
    except Exception as e:
        return {"error": f"Predictions not available: {str(e)}"}

@app.post("/analytics/session/start")
def start_session(user_id: str = None):
    """Start a new logging session"""
    try:
        import uuid
        session_id = str(uuid.uuid4())
        return {"session_id": session_id, "user_id": user_id}
    except Exception as e:
        return {"error": f"Session start failed: {str(e)}"}

@app.post("/analytics/session/{session_id}/end")
def end_session(session_id: str):
    """End a logging session"""
    try:
        return {"message": "Session ended", "session_id": session_id}
    except Exception as e:
        return {"error": f"Session end failed: {str(e)}"}

# Performance benchmarking endpoint
@app.get("/benchmark/run")
def run_performance_benchmark():
    """Run performance benchmark"""
    try:
        from dev.scripts.model_performance_report import ModelPerformanceBenchmark
        benchmark = ModelPerformanceBenchmark(api_url="http://localhost:8000")
        results = benchmark.run_comprehensive_benchmark()
        return results
    except Exception as e:
        return {"error": f"Benchmark failed: {str(e)}"}

# Streaming test page
@app.post("/predict/batch",
    summary="Batch Sentiment Analysis",
    description="""
    Process multiple inputs in a single request for efficient batch analysis.

    **Supported Batch Types:**
    - Multiple text inputs
    - Multiple file uploads (audio/video)
    - Mixed multimodal inputs

    **Advanced Features:**
    - Parallel processing for optimal performance
    - Individual confidence scores and model versions
    - Aggregated batch statistics
    - Error handling for individual items

    **Response includes:**
    - Individual results for each input
    - Batch-level statistics and insights
    - Processing performance metrics
    """,
    response_description="Batch sentiment analysis results with comprehensive metrics")
async def predict_batch(
    texts: List[str] = Form(None),
    files: List[UploadFile] = File(None)
):
    """Advanced batch processing for multiple inputs"""
    if not texts and not files:
        raise HTTPException(status_code=400, detail="At least one text or file input is required")

    start_time = time.time()
    results = []
    batch_stats = {
        'total_items': 0,
        'successful': 0,
        'failed': 0,
        'sentiment_distribution': {'positive': 0, 'negative': 0, 'neutral': 0},
        'average_confidence': 0.0,
        'processing_time_ms': 0
    }

    # Process text inputs
    if texts:
        for i, text in enumerate(texts):
            try:
                sanitized_text = input_validator.validate_text_input(text)
                analysis_result = get_text_model().predict(sanitized_text)

                if isinstance(analysis_result, dict):
                    sentiment = analysis_result.get('sentiment', 'neutral')
                    confidence = analysis_result.get('confidence', 0.5)
                else:
                    sentiment, confidence = analysis_result

                results.append({
                    'index': i,
                    'type': 'text',
                    'input_preview': text[:100] + '...' if len(text) > 100 else text,
                    'sentiment': sentiment,
                    'confidence': confidence,
                    'status': 'success'
                })

                batch_stats['successful'] += 1
                batch_stats['sentiment_distribution'][sentiment] += 1

            except Exception as e:
                results.append({
                    'index': i,
                    'type': 'text',
                    'error': str(e),
                    'status': 'failed'
                })
                batch_stats['failed'] += 1

    # Calculate batch statistics
    batch_stats['total_items'] = len(results)
    successful_results = [r for r in results if r['status'] == 'success']
    if successful_results:
        batch_stats['average_confidence'] = sum(r.get('confidence', 0) for r in successful_results) / len(successful_results)

    processing_time = time.time() - start_time
    batch_stats['processing_time_ms'] = processing_time * 1000

    return format_api_response(
        sentiment='batch_analysis',
        confidence=batch_stats['average_confidence'],
        used_models=['batch_processor'],
        processing_time=processing_time * 1000,
        batch_results=results,
        batch_statistics=batch_stats
    )

@app.get("/streaming/test", response_class=HTMLResponse)
def get_streaming_test():
    """Serve streaming test page"""
    return HTMLResponse(content=STREAMING_TEST_HTML)

# Add streaming routes
add_streaming_routes(app)

# ============================================================================
# FUSION CONFIGURATION MANAGEMENT API ENDPOINTS (Day 3 Requirement)
# ============================================================================

@app.get("/config/fusion",
    summary="Get Current Fusion Configuration",
    description="""
    Get the current fusion configuration settings.

    **Day 3 Feature:** Runtime configuration access for teams to view current settings.

    **Returns:**
    - Complete fusion configuration
    - Current weights and method
    - Team presets available
    """,
    response_description="Current fusion configuration")
async def get_fusion_config():
    """Get current fusion configuration"""
    try:
        config_manager = get_fusion_config_manager()
        config = config_manager.load_config()

        return {
            "status": "success",
            "config": config,
            "current_method": config_manager.get_fusion_method(),
            "current_weights": config_manager.get_weights(),
            "available_presets": list(config.get('fusion', {}).get('team_presets', {}).keys()),
            "timestamp": time.time()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving fusion config: {str(e)}")

@app.post("/config/fusion/weights",
    summary="Update Fusion Weights",
    description="""
    Update the fusion weights for different modalities at runtime.

    **Day 3 Feature:** Dynamic weight adjustment without restart.

    **Request Body:**
    ```json
    {
        "text": 0.5,
        "audio": 0.3,
        "video": 0.2
    }
    ```

    **Validation:**
    - Weights must sum to 1.0
    - Each weight must be between 0.0 and 1.0
    """,
    response_description="Updated fusion configuration")
async def update_fusion_weights(weights: dict):
    """Update fusion weights at runtime"""
    try:
        config_manager = get_fusion_config_manager()

        # Validate weights
        if not isinstance(weights, dict):
            raise HTTPException(status_code=400, detail="Weights must be a dictionary")

        required_keys = {'text', 'audio', 'video'}
        if not required_keys.issubset(weights.keys()):
            raise HTTPException(status_code=400, detail=f"Missing required weight keys: {required_keys - weights.keys()}")

        # Check weight values
        for key, value in weights.items():
            if not isinstance(value, (int, float)) or value < 0 or value > 1:
                raise HTTPException(status_code=400, detail=f"Weight '{key}' must be between 0.0 and 1.0")

        # Check sum equals 1.0 (with tolerance)
        weight_sum = sum(weights.values())
        if abs(weight_sum - 1.0) > 0.001:
            raise HTTPException(status_code=400, detail=f"Weights must sum to 1.0, got {weight_sum}")

        # Update weights
        success = config_manager.update_weights(weights)
        if not success:
            raise HTTPException(status_code=500, detail="Failed to update weights")

        return {
            "status": "success",
            "message": "Fusion weights updated successfully",
            "new_weights": weights,
            "timestamp": time.time()
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error updating fusion weights: {str(e)}")

@app.post("/config/fusion/method",
    summary="Update Fusion Method",
    description="""
    Update the fusion method at runtime.

    **Day 3 Feature:** Switch fusion algorithms without restart.

    **Available Methods:**
    - `simple`: Basic weighted average
    - `confidence_weighted`: Dynamic confidence-based weighting
    - `adaptive`: Adaptive learning-based fusion

    **Request Body:**
    ```json
    {
        "method": "confidence_weighted"
    }
    ```
    """,
    response_description="Updated fusion method")
async def update_fusion_method(method_data: dict):
    """Update fusion method at runtime"""
    try:
        config_manager = get_fusion_config_manager()

        if not isinstance(method_data, dict) or 'method' not in method_data:
            raise HTTPException(status_code=400, detail="Request must contain 'method' field")

        method = method_data['method']
        valid_methods = ['simple', 'confidence_weighted', 'adaptive', 'custom']

        if method not in valid_methods:
            raise HTTPException(status_code=400, detail=f"Invalid method. Must be one of: {valid_methods}")

        success = config_manager.update_method(method)
        if not success:
            raise HTTPException(status_code=500, detail="Failed to update fusion method")

        return {
            "status": "success",
            "message": f"Fusion method updated to '{method}'",
            "new_method": method,
            "timestamp": time.time()
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error updating fusion method: {str(e)}")

@app.post("/config/fusion/preset/{team_name}",
    summary="Apply Team Preset Configuration",
    description="""
    Apply a team-specific fusion configuration preset.

    **Day 3 Feature:** Quick configuration switching for different teams.

    **Available Teams:**
    - `gandhar_avatar_emotions`: Optimized for avatar emotion detection
    - `vedant_teacher_scoring`: Optimized for AI teacher scoring
    - `shashank_content_moderation`: Optimized for content moderation

    **Path Parameters:**
    - `team_name`: Name of the team preset to apply
    """,
    response_description="Applied team preset configuration")
async def apply_team_preset(team_name: str):
    """Apply team-specific configuration preset"""
    try:
        config_manager = get_fusion_config_manager()

        # Get available presets
        available_presets = config_manager.get_team_preset(None)  # Get all presets
        if not available_presets or team_name not in available_presets:
            available = list(available_presets.keys()) if available_presets else []
            raise HTTPException(
                status_code=404,
                detail=f"Team preset '{team_name}' not found. Available presets: {available}"
            )

        success = config_manager.apply_team_preset(team_name)
        if not success:
            raise HTTPException(status_code=500, detail=f"Failed to apply preset for team '{team_name}'")

        # Get the applied configuration
        preset_config = config_manager.get_team_preset(team_name)

        return {
            "status": "success",
            "message": f"Applied configuration preset for team '{team_name}'",
            "team": team_name,
            "applied_config": preset_config,
            "timestamp": time.time()
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error applying team preset: {str(e)}")

@app.get("/config/fusion/presets",
    summary="List Available Team Presets",
    description="""
    Get all available team configuration presets.

    **Day 3 Feature:** Discovery of available team configurations.

    **Returns:**
    - List of available team presets
    - Configuration details for each preset
    - Integration notes for teams
    """,
    response_description="Available team presets")
async def list_team_presets():
    """List all available team configuration presets"""
    try:
        config_manager = get_fusion_config_manager()
        config = config_manager.load_config()

        presets = config.get('fusion', {}).get('team_presets', {})
        integration_notes = config.get('integration_notes', {})

        return {
            "status": "success",
            "presets": presets,
            "integration_notes": integration_notes,
            "count": len(presets),
            "timestamp": time.time()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error retrieving team presets: {str(e)}")

@app.post("/config/fusion/reload",
    summary="Reload Fusion Configuration",
    description="""
    Reload fusion configuration from the YAML file.

    **Day 3 Feature:** Manual configuration reload without restart.

    **Use Cases:**
    - After manual YAML file edits
    - To refresh configuration from external updates
    - Troubleshooting configuration issues
    """,
    response_description="Configuration reload status")
async def reload_fusion_config():
    """Reload fusion configuration from file"""
    try:
        config_manager = get_fusion_config_manager()
        config = config_manager.load_config()

        return {
            "status": "success",
            "message": "Fusion configuration reloaded successfully",
            "config_timestamp": time.time(),
            "current_method": config_manager.get_fusion_method(),
            "current_weights": config_manager.get_weights()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error reloading fusion config: {str(e)}")

# Mount analytics dashboard as sub-application
from advanced_analytics_dashboard import analytics_app
app.mount("/analytics", analytics_app)

# Add analytics dashboard route
@app.get("/analytics-dashboard", response_class=HTMLResponse)
async def analytics_dashboard_redirect():
    """Redirect to analytics dashboard"""
    return """
    <html>
        <head>
            <meta http-equiv="refresh" content="0; url=/analytics/">
            <title>Redirecting to Analytics Dashboard</title>
        </head>
        <body>
            <p>Redirecting to <a href="/analytics/">Analytics Dashboard</a>...</p>
        </body>
    </html>
    """

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)